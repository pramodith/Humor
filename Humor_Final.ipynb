{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Humor_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnkAiz9CYrm/rCK5meJbTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pramodith/Humor/blob/master/Humor_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efFQuJZqXwY8",
        "colab_type": "code",
        "outputId": "6fc8cdba-cab6-4cc3-fcf4-b334a6705f6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%bash\n",
        "pip install pytorch-pretrained-bert\n",
        "pip install tqdm boto3 requests regex\n",
        "pip install pytorch-pretrained-bert pytorch-nlp\n",
        "pip install sacremoses\n",
        "pip install sentencepiece\n",
        "pip install pytorch_transformers\n",
        "pip install transformers\n",
        "pip install gensim\n",
        "pip install spacy\n",
        "python -m spacy download en_core_web_lg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.5.0+cu101)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.12.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.15.46)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.46->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (1.12.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3) (1.15.46)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.9.5)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.46->boto3) (1.12.0)\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.38.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.12.46)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.5.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.15.46)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.46->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (0.0.41)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.38.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.14.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.86)\n",
            "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.38.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.5.0+cu101)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.0.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.18.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.1.86)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.12.46)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch_transformers) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.15.46)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->pytorch_transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->pytorch_transformers) (0.15.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.46)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.12.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.15.46)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.38.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0YGymLjwoqA",
        "colab_type": "text"
      },
      "source": [
        "This notebook, is to reproduce the work of Pramodith pertaining to SemEval2020 Task 7. Assessing the Humor of Edited News Headlines. All required data files can be found in the git repo in which thie jupyter notebook is located. I know that the code hasn't been maintained properly since the work was done as an independent researcher in my spare time. If you have any doubts feel free to raise a GIT issue.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lI0ZR9ibDip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import json\n",
        "import re\n",
        "import gensim\n",
        "from transformers import BertTokenizer,DistilBertTokenizer,RobertaTokenizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8etXyG4rBvZ",
        "colab_type": "text"
      },
      "source": [
        "Get the tokenized input and the locations of the focus words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXxmALIeYJy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_bert(X: list, org : bool,tokenizer_type='roberta'):\n",
        "    '''\n",
        "    This function tokenizes the input sentences and returns a vectorized representation of them and the location\n",
        "    of each entity in the sentence.\n",
        "\n",
        "    :param X: List of all input sentences\n",
        "    :return: A vectorized list representation of the sentence and a numpy array containing the locations of each entity. First two\n",
        "    values in  a row belong to entity1 and the next two values belong to entity2.\n",
        "    '''\n",
        "\n",
        "    # Add the SOS and EOS tokens.\n",
        "    # TODO: Replace fullstops with [SEP]\n",
        "    if tokenizer_type == 'roberta':\n",
        "      tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "      sentences = [\"<s> \" + sentence + \" </s>\" for sentence in X]\n",
        "    elif tokenizer_type == 'bert':\n",
        "      tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "      sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in X]\n",
        "    elif tokenizer_type == 'distilbert':\n",
        "      tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "      sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in X]\n",
        "    \n",
        "\n",
        "    # Tokenize and vectorize\n",
        "    tokenized_text = [tokenizer.tokenize(sentence,add_special_tokens=False) for sentence in sentences]\n",
        "    X = [tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_text]\n",
        "    print(tokenizer_type)\n",
        "    print(tokenized_text[0])\n",
        "    print(tokenizer.pad_token)\n",
        "    print(tokenizer.pad_token_id)\n",
        "\n",
        "    # MAX_SEQ_LEN\n",
        "    MAX_LEN = 50\n",
        "    #Pad sequences to make them all eqally long\n",
        "    X = pad_sequences(X, MAX_LEN, 'long', 'post', 'post',value=tokenizer.pad_token_id)\n",
        "    print(X[0])\n",
        "    \n",
        "    if tokenizer_type != 'roberta':\n",
        "      if org:\n",
        "          entity_locs = np.asarray([[i for i, s in enumerate(sent) if s == '<'] for sent in tokenized_text])\n",
        "      else:\n",
        "          entity_locs = np.asarray([[i for i, s in enumerate(sent) if s == '^'] for sent in tokenized_text])\n",
        "    \n",
        "    else:\n",
        "      # Find the locations of each entity and store them\n",
        "      if org:\n",
        "          entity_locs = np.asarray([[i for i, s in enumerate(sent) if '<' in s and len(s)==2] for sent in tokenized_text])\n",
        "      else:\n",
        "          entity_locs = np.asarray([[i for i, s in enumerate(sent) if '^' in s and len(s)==2] for sent in tokenized_text])\n",
        "    print(entity_locs[0]) \n",
        "    return X,entity_locs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgqR7gHBrNyb",
        "colab_type": "text"
      },
      "source": [
        "This function is to get the dataloaders to repeat the experiment of using a Non-Siamese network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD_741VUVAQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sent_emb_dataloaders_bert(file_path: str, mode='train', train_batch_size=64, test_batch_size=64, model=None):\n",
        "    \n",
        "    df = pd.read_csv(file_path, sep=\",\")\n",
        "    # Get the additional data.\n",
        "    if mode == 'train':\n",
        "        df1 = pd.read_csv(file_path[:-4] + \"_funlines.csv\", sep=\",\")\n",
        "        df = pd.concat([df, df1], ignore_index=True)\n",
        "    id = df['id']\n",
        "    X = df['original'].values\n",
        "    X = [sent.replace(\"\\\"\", \"\") for sent in X]\n",
        "    # Replaced word\n",
        "    replaced = df['original'].apply(lambda x: x[x.index(\"<\"):x.index(\">\") + 1])\n",
        "    replaced_clean = [x.replace(\"<\", \"\").replace(\"/>\", \"\") for x in replaced]\n",
        "    if mode != 'test':\n",
        "        y = df['meanGrade'].values\n",
        "    edit = df['edit']\n",
        "    # Substitute the edit word in the place of the replaced word add the required demarcation tokens.\n",
        "    X2 = [sent.replace(replaced[i], \"^ \" + edit[i] + \" ^\") for i, sent in enumerate(X)]\n",
        "    X1 = [sent.replace(\"<\", \"< \").replace(\"/>\", \" <\") for i, sent in enumerate(X)]\n",
        "    X, entity_locs = tokenize_roberta_sent(X1, X2)\n",
        "\n",
        "    if mode == \"train\":\n",
        "\n",
        "        train1_inputs = torch.tensor(X)\n",
        "        train_labels = torch.tensor(y)\n",
        "        train_entity_locs = torch.tensor(entity_locs)\n",
        "\n",
        "        # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n",
        "        # with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "        # train_data = TensorDataset(train1_inputs,train2_inputs, train_entity_locs, train_word2vec_locs, train_labels)\n",
        "        train_data = TensorDataset(train1_inputs, train_entity_locs, train_labels)\n",
        "        train_dataloader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "        # validation_data = TensorDataset(validation1_inputs,validation2_inputs, validation_entity_locs, validation_word2vec_locs, validation_labels)\n",
        "        return train_dataloader\n",
        "\n",
        "    if mode == \"val\":\n",
        "        test1_input = torch.tensor(X)\n",
        "        y = torch.tensor(y)\n",
        "        train_entity_locs = torch.tensor(entity_locs)\n",
        "        # word2vec_locs = torch.tensor(word2vec_indices)\n",
        "        id = torch.tensor(id)\n",
        "        test_data = TensorDataset(test1_input, train_entity_locs,y, id)\n",
        "        test_sampler = SequentialSampler(test_data)\n",
        "        test_data_loader = DataLoader(test_data, sampler=test_sampler, batch_size=test_batch_size)\n",
        "\n",
        "        return test_data_loader\n",
        "\n",
        "    if mode == \"test\":\n",
        "        test1_input = torch.tensor(X)\n",
        "        test2_input = torch.tensor(sent_emb)\n",
        "\n",
        "        train_entity_locs = torch.tensor(entity_locs)\n",
        "        # word2vec_locs = torch.tensor(word2vec_indices)\n",
        "        id = torch.tensor(id)\n",
        "        test_data = TensorDataset(test1_input, test2_input, train_entity_locs, id)\n",
        "        test_sampler = SequentialSampler(test_data)\n",
        "        test_data_loader = DataLoader(test_data, sampler=test_sampler, batch_size=test_batch_size)\n",
        "\n",
        "        return test_data_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K78_i5xZrwYD",
        "colab_type": "text"
      },
      "source": [
        "Get the data loader for language modeling assuming that the model is roberta.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahFdzd4D9tVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_bert_lm_dataloader(file_path : str,batch_size = 16):\n",
        "    jokes_df = pd.read_csv(file_path)\n",
        "    jokes = jokes_df['Joke']\n",
        "    jokes = \"<s> \" + jokes + \" </s>\"\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    X = [tokenizer.encode(sent,add_special_tokens=False) for sent in jokes]\n",
        "    MAX_LEN = max([len(sent) for sent in X])\n",
        "    print(MAX_LEN)\n",
        "    X = pad_sequences(X, MAX_LEN, 'long', 'post', 'post',tokenizer.pad_token_id)\n",
        "    dataset = TensorDataset(torch.tensor(X))\n",
        "    sampler = RandomSampler(dataset)\n",
        "    data_loader = DataLoader(dataset, sampler=sampler, batch_size=batch_size,pin_memory=True)\n",
        "    return data_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDqA54Vqr-M0",
        "colab_type": "text"
      },
      "source": [
        "This function is to get the data loaders for the Siamese architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ8Q9tXrZ0IO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataloaders_bert(file_path : str,model_type,mode=\"train\",train_batch_size=64,test_batch_size = 64):\n",
        "\n",
        "    '''\n",
        "    This function creates pytorch dataloaders for fast and easy iteration over the dataset.\n",
        "\n",
        "    :param file_path: Path of the file containing train/test data\n",
        "    :param mode: Test mode or Train mode\n",
        "    :param train_batch_size: Size of the batch during training\n",
        "    :param test_batch_size: Size of the batch during testing\n",
        "    :return: Dataloaders\n",
        "    '''\n",
        "\n",
        "    # Read the data,tokenize and vectorize\n",
        "    df = pd.read_csv(file_path, sep=\",\")\n",
        "    if mode=='train':\n",
        "        df1 = pd.read_csv(file_path[:-4]+\"_funlines.csv\",sep=\",\")\n",
        "        df = pd.concat([df,df1],ignore_index=True)\n",
        "    id = df['id']\n",
        "    X = df['original'].values\n",
        "    X = [sent.replace(\"\\\"\",\"\") for sent in X]\n",
        "    \n",
        "    replaced = df['original'].apply(lambda x: x[x.index(\"<\"):x.index(\">\")+1])\n",
        "    replaced_clean = [x.replace(\"<\",\"\").replace(\"/>\",\"\") for x in replaced]\n",
        "    if mode!='test':\n",
        "        y = df['meanGrade'].values\n",
        "    edit = df['edit']\n",
        "    X2 = [sent.replace(replaced[i], \"^ \" + edit[i] + \" ^\") for i, sent in enumerate(X)]\n",
        "    X1 = [sent.replace(\"<\",\"< \").replace(\"/>\",\" <\") for i,sent in enumerate(X)]\n",
        "    X1,e1_locs = tokenize_bert(X1,True,model_type)\n",
        "    X2,e2_locs = tokenize_bert(X2,False,model_type)\n",
        "\n",
        "    replacement_locs = np.concatenate((e1_locs, e2_locs), 1)\n",
        "    \n",
        "    if mode == \"train\":\n",
        "\n",
        "\n",
        "        train1_inputs = torch.tensor(X1)\n",
        "        train2_inputs = torch.tensor(X2)\n",
        "        train_labels = torch.tensor(y)\n",
        "        train_entity_locs = torch.tensor(replacement_locs)\n",
        "        \n",
        "        # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n",
        "        # with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "        #train_data = TensorDataset(train1_inputs,train2_inputs, train_entity_locs, train_word2vec_locs, train_labels)\n",
        "        train_data = TensorDataset(train1_inputs, train2_inputs, train_entity_locs, train_labels)\n",
        "        train_sampler = RandomSampler(train_data)\n",
        "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
        "\n",
        "        return  train_dataloader\n",
        "\n",
        "    if mode == \"val\":\n",
        "        test1_input = torch.tensor(X1)\n",
        "        test2_input = torch.tensor(X2)\n",
        "\n",
        "        train_entity_locs = torch.tensor(replacement_locs)\n",
        "        y = torch.tensor(y)\n",
        "        id = torch.tensor(id)\n",
        "        test_data = TensorDataset(test1_input, test2_input, train_entity_locs,y,id)\n",
        "        test_sampler = SequentialSampler(test_data)\n",
        "        test_data_loader = DataLoader(test_data, sampler=test_sampler, batch_size=test_batch_size)\n",
        "        return test_data_loader\n",
        "\n",
        "    if mode == \"test\":\n",
        "        test1_input = torch.tensor(X1)\n",
        "        test2_input = torch.tensor(X2)\n",
        "\n",
        "        train_entity_locs = torch.tensor(replacement_locs)\n",
        "        id = torch.tensor(id)\n",
        "        test_data = TensorDataset(test1_input, test2_input, train_entity_locs,id)\n",
        "        test_sampler = SequentialSampler(test_data)\n",
        "        test_data_loader = DataLoader(test_data, sampler=test_sampler, batch_size=test_batch_size)\n",
        "\n",
        "        return test_data_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za29BKq2sOLT",
        "colab_type": "text"
      },
      "source": [
        "Tokenizer function to be used by non-siamese architecture assuming transformer model is Roberta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwXwY-LQaY15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_roberta_sent(X1: list, X2 : list ):\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    sentences = [\"<s> \" + X1[i] + \" </s></s> \" + X2[i] + \" </s>\" for i in range(len(X1))]\n",
        "    tokenized_text = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
        "    print(tokenized_text[0])\n",
        "    X = [tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_text]\n",
        "    print(X[0])\n",
        "    #sent_emb = [[0 if i<sentence.index(102) else 1 for i  in range(len(sentence)) ] for sentence in X]\n",
        "    MAX_LEN = max([len(x) for x in X])+1\n",
        "    print(MAX_LEN)\n",
        "    # Pad sequences to make them all eqally long\n",
        "    X = pad_sequences(X, MAX_LEN, 'long', 'post', 'post',tokenizer.pad_token_id)\n",
        "    #sent_emb = pad_sequences(sent_emb,MAX_LEN,'long','post','post',1)\n",
        "    # Find the locations of each entity and store them\n",
        "    entity_locs1 = np.asarray(\n",
        "            [[i for i, s in enumerate(sent) if '<' in s and len(s) == 2] for sent in tokenized_text])\n",
        "    entity_locs2 = np.asarray([[i for i, s in enumerate(sent) if '^' in s and len(s) == 2] for sent in tokenized_text])\n",
        "\n",
        "    return X,np.concatenate((entity_locs1, entity_locs2), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET0bfkZUsNfJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFzqBDMnbIFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
        "import sys\n",
        "from transformers.optimization import AdamW,get_linear_schedule_with_warmup\n",
        "from transformers import BertForMaskedLM, DistilBertForMaskedLM, RobertaModel, BertModel,DistilBertModel,RobertaForMaskedLM\n",
        "from pytorch_pretrained_bert import BertAdam\n",
        "import argparse\n",
        "import torchnlp.nn as nn_nlp\n",
        "import gensim\n",
        "import numpy as np\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxFMVGFibM0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(12)\n",
        "torch.cuda.manual_seed(12)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXQXJZinbP9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FunBERT(nn.Module):\n",
        "\n",
        "    def __init__(self, train_file_path: str, dev_file_path: str, test_file_path: str, lm_file_path: str,\n",
        "                 train_batch_size: int,\n",
        "                 test_batch_size: int, lr: float, lm_weights_file_path: str, epochs: int, lm_pretrain: str,\n",
        "                 model_path: str,model_type : str):\n",
        "      '''\n",
        "\n",
        "      :param train_file_path: Path to the train file\n",
        "      :param test_file_path: Path to the test file\n",
        "      :param train_batch_size: Size of the batch during training\n",
        "      :param test_batch_size: Size of the batch during testing\n",
        "      :param lr: learning rate\n",
        "      '''\n",
        "\n",
        "      super(FunBERT, self).__init__()\n",
        "      if lm_pretrain and model_type == 'roberta':\n",
        "        self.bert_model = RobertaForMaskedLM.from_pretrained('roberta-base', output_hidden_states=True)\n",
        "        self.tokenizer= RobertaTokenizer.from_pretrained('roberta-base')\n",
        "      elif model_type == 'roberta':\n",
        "        self.bert_model = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\n",
        "        self.tokenizer= RobertaTokenizer.from_pretrained('roberta-base')\n",
        "      elif model_type == 'bert':\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "        self.tokenizer= BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "      elif model_type == 'distilbert':\n",
        "        self.bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased',output_hidden_states=True)\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "      self.model_type = model_type\n",
        "      self.train_batch_size = train_batch_size\n",
        "      self.test_batch_size = test_batch_size\n",
        "      self.train_file_path = train_file_path\n",
        "      self.lm_file_path = lm_file_path\n",
        "      self.attention = nn_nlp.Attention(768 * 2)\n",
        "      self.dev_file_path = dev_file_path\n",
        "      self.test_file_path = test_file_path\n",
        "      self.lr = lr\n",
        "      \n",
        "      self.prelu = nn.PReLU()\n",
        "      self.epochs = epochs\n",
        "      self.linear_reg1 = nn.Sequential(\n",
        "          nn.Dropout(0.3),\n",
        "          nn.Linear(768 * 8, 1024))\n",
        "\n",
        "      self.final_linear = nn.Sequential(nn.Dropout(0.3), nn.Linear(1024, 1))\n",
        "    \n",
        "    def pre_train_bert(self):\n",
        "        optimizer = optim.Adam(self.bert_model.parameters(), 2e-5)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, 62,620)\n",
        "        step = 0\n",
        "        train_dataloader = get_bert_lm_dataloader(self.lm_file_path, 32)\n",
        "        print(\"Training LM\")\n",
        "        if torch.cuda.is_available():\n",
        "            self.bert_model.cuda()\n",
        "        for epoch in range(2):\n",
        "            print(\"Epoch : \" + str(epoch))\n",
        "            for ind, batch in enumerate(train_dataloader):\n",
        "                step += 1\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                if torch.cuda.is_available():\n",
        "                    inp = batch[0].cuda()\n",
        "                else:\n",
        "                    inp = batch[0]\n",
        "\n",
        "                labels = inp.clone()\n",
        "                # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "                probability_matrix = torch.full(labels.shape, 0.15)\n",
        "                special_tokens_mask = [\n",
        "                    self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in\n",
        "                    labels.tolist()\n",
        "                ]\n",
        "                probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "                if self.tokenizer._pad_token is not None:\n",
        "                    padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
        "                    padding_mask = padding_mask.detach().cpu()\n",
        "                    probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
        "                masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "                labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "                # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "                indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "                inp[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
        "\n",
        "                # 10% of the time, we replace masked input tokens with random word\n",
        "                indices_random = torch.bernoulli(\n",
        "                    torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "                random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
        "                inp[indices_random] = random_words[indices_random].cuda()\n",
        "                outputs = self.bert_model(inp, masked_lm_labels=labels.long(),attention_mask=(inp!=self.tokenizer.pad_token_id).long())\n",
        "                loss, prediction_scores = outputs[:2]\n",
        "                loss.backward()\n",
        "                #torch.nn.utils.clip_grad_norm_(self.bert_model.parameters(), 1.0)\n",
        "                print(str(step) + \" Loss is :\" + str(loss.item()))\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "            torch.cuda.empty_cache()\n",
        "        print(\"LM training done\")\n",
        "        torch.save(self.bert_model.state_dict(), \"lm_joke_bert.pth\")\n",
        "\n",
        "    # Use this forward for non-siamese model.\n",
        "    def forward1(self, *input) :\n",
        "        final_out = []\n",
        "        input = input[0]\n",
        "        attn_mask0 = (input[0] != self.tokenizer.pad_token_id).long()\n",
        "        out_per_seq, _,attention_layer_inps= self.bert_model(input[0].long(),attention_mask=attn_mask0)\n",
        "        out_per_seq = torch.cat((out_per_seq,attention_layer_inps[11]),2)\n",
        "        pos = input[0].clone().detach().cpu()\n",
        "        for (i, loc) in enumerate(input[1]):\n",
        "            # +1 is to ensure that the symbol token is not considered\n",
        "            entity1 = torch.mean(out_per_seq[i,loc[0]+1:loc[1]],0)\n",
        "            entity2 = torch.mean(out_per_seq[i, loc[2] + 1:loc[3]], 0)\n",
        "            # Limit attention to original sentence for entity1 and edited sentence for entity2 \n",
        "            imp_seq1 = torch.cat((out_per_seq[i, 0:loc[0] + 1], out_per_seq[i, loc[1]:np.where(pos[i].numpy()==self.tokenizer.sep_token_id)[0][0]]), 0)\n",
        "            imp_seq2 = torch.cat((out_per_seq[i, np.where(pos[i].numpy()==self.tokenizer.sep_token_id)[0][1]:loc[2] + 1], out_per_seq[i, loc[3]:]), 0)\n",
        "            _, attention_score = self.attention(entity2.unsqueeze(0).unsqueeze(0), imp_seq2.unsqueeze(0))\n",
        "            sent_attn2 = torch.sum(attention_score.squeeze(0).expand(768 * 2, -1).t() * imp_seq2, 0)\n",
        "            _, attention_score = self.attention(entity1.unsqueeze(0).unsqueeze(0), imp_seq1.unsqueeze(0))\n",
        "            sent_attn1 = torch.sum(attention_score.squeeze(0).expand(768 * 2, -1).t() * imp_seq1, 0)\n",
        "            #attn_diff = torch.abs(sent_attn2-sent_attn1)\n",
        "            sent_out = self.prelu(self.linear_reg1(torch.cat((sent_attn2,sent_attn1,out_per_seq[i,0],entity2), 0)))\n",
        "            out = self.final_linear(sent_out)\n",
        "            final_out.append(out)\n",
        "        #out = self.final_linear(torch.cat((out_per_seq[:, 0, :],entity_diff), 1))\n",
        "\n",
        "        return torch.stack(final_out)\n",
        "\n",
        "    def forward(self, *input):\n",
        "        '''\n",
        "        :param input: input[0] is the sentence, input[1] are the entity locations , input[2] is the ground truth\n",
        "        :return: Scores for each class\n",
        "        '''\n",
        "\n",
        "        final_scores = []\n",
        "        input = input[0]\n",
        "        attn_mask0 = (input[0]!=self.tokenizer.pad_token_id).long()\n",
        "        output_per_seq1,_,attention_layer_inps = self.bert_model(input[0].long(), attention_mask = attn_mask0)\n",
        "        output_per_seq1 = torch.cat((output_per_seq1, attention_layer_inps[11]), 2)\n",
        "        attn_mask1 = (input[1]!=self.tokenizer.pad_token_id).long()\n",
        "        output_per_seq2,_,attention_layer_inps = self.bert_model(input[1].long(),attention_mask = attn_mask1)\n",
        "        output_per_seq2 = torch.cat((output_per_seq2, attention_layer_inps[11]), 2)\n",
        "        '''\n",
        "        Obtain the vectors that represent the entities and average them followed by a Tanh and a linear layer.\n",
        "        '''\n",
        "        for (i, loc) in enumerate(input[2]):\n",
        "            # +1 is to ensure that the symbol token is not considered\n",
        "            \n",
        "            entity1 = torch.mean(output_per_seq1[i, loc[0] + 1:loc[1]], 0)\n",
        "            entity2 = torch.mean(output_per_seq2[i, loc[2] + 1:loc[3]], 0)\n",
        "            \n",
        "            imp_seq1 = torch.cat((output_per_seq1[i, 0:loc[0] + 1], output_per_seq1[i, loc[1]:]), 0)\n",
        "            imp_seq2 = torch.cat((output_per_seq2[i, 0:loc[2] + 1], output_per_seq2[i, loc[3]:]), 0)\n",
        "            _, attention_score = self.attention(entity2.unsqueeze(0).unsqueeze(0), imp_seq2.unsqueeze(0))\n",
        "            sent_attn = torch.sum(attention_score.squeeze(0).expand(768 * 2, -1).t() * imp_seq2, 0)\n",
        "            _, attention_score1 = self.attention(entity1.unsqueeze(0).unsqueeze(0), imp_seq1.unsqueeze(0))\n",
        "            sent_attn1 = torch.sum(attention_score1.squeeze(0).expand(768 * 2, -1).t() * imp_seq1, 0)\n",
        "            sent_out = self.prelu(self.linear_reg1(torch.cat((sent_attn,sent_attn1,output_per_seq2[i,0],entity2), 0)))\n",
        "            final_out = self.final_linear(sent_out)\n",
        "            final_scores.append(final_out)\n",
        "        \n",
        "        return torch.stack((final_scores))\n",
        "    \n",
        "    def train_non_siamese(self):\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=0.001)\n",
        "\n",
        "        loss = nn.MSELoss()\n",
        "        train_dataloader = get_sent_emb_dataloaders_bert(self.train_file_path,'train',self.train_batch_size)\n",
        "\n",
        "        val_dataloader = get_sent_emb_dataloaders_bert(self.dev_file_path,\"val\",self.train_batch_size)\n",
        "        best_loss = sys.maxsize\n",
        "        best_accuracy = -sys.maxsize\n",
        "        steps = 0\n",
        "        pred_scores = []\n",
        "        gt_scores = []\n",
        "        print(f\"Pad token is {self.tokenizer.pad_token}\")\n",
        "        for epoch in range(self.epochs):\n",
        "            steps += 1\n",
        "            if epoch == 0:\n",
        "                scheduler = get_linear_schedule_with_warmup(optimizer,140,1400)\n",
        "            total_prev_loss = 0\n",
        "            for (batch_num, batch) in enumerate(train_dataloader):\n",
        "                # If gpu is available move to gpu.\n",
        "                if torch.cuda.is_available():\n",
        "                    input1 = batch[0].cuda()\n",
        "                    locs = batch[1].cuda()\n",
        "                    gt = batch[2].cuda()\n",
        "                else:\n",
        "                    input1 = batch[0]\n",
        "                    locs = batch[1]\n",
        "                    gt = batch[2]\n",
        "\n",
        "                loss_val = 0\n",
        "                self.bert_model.train()\n",
        "                self.attention.train()\n",
        "                self.linear_reg1.train()\n",
        "                self.prelu.train()\n",
        "                self.final_linear.train()\n",
        "\n",
        "                # Clear gradients\n",
        "                optimizer.zero_grad()\n",
        "                final_scores = self.forward1((input1,locs))\n",
        "                loss_val += loss(final_scores.squeeze(1), gt.float())\n",
        "                \n",
        "\n",
        "                # Compute gradients\n",
        "                loss_val.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
        "                total_prev_loss += loss_val.item()\n",
        "                print(\"Loss for batch\" + str(batch_num) + \": \" + str(loss_val.item()))\n",
        "                # Update weights according to the gradients computed.\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            # Don't compute gradients in validation step\n",
        "            with torch.no_grad():\n",
        "                # Ensure that dropout behavior is correct.\n",
        "                pred_scores = []\n",
        "                gt_scores = []\n",
        "                predictions = []\n",
        "                ground_truth = []\n",
        "                self.bert_model.eval()\n",
        "                self.attention.eval()\n",
        "                self.linear_reg1.eval()\n",
        "                self.final_linear.eval()\n",
        "                self.prelu.eval()\n",
        "                mse_loss = 0\n",
        "                for (val_batch_num, val_batch) in enumerate(val_dataloader):\n",
        "                    if torch.cuda.is_available():\n",
        "                        input1 = val_batch[0].cuda()\n",
        "                        locs = val_batch[1].cuda()\n",
        "                        gt = val_batch[2].cuda()\n",
        "                    else:\n",
        "                        input1 = val_batch[0]\n",
        "                        locs = val_batch[1]\n",
        "                        gt = val_batch[2]\n",
        "\n",
        "                    final_scores = self.forward1((input1, locs))\n",
        "                    pred_scores.extend(final_scores.cpu().detach().squeeze(1))\n",
        "                    gt_scores.extend(gt.cpu().detach())\n",
        "\n",
        "                    mse_loss += mean_squared_error(gt.cpu().detach(),final_scores.cpu().detach().squeeze(1))\n",
        "\n",
        "                    \n",
        "                print(f\"Validation Loss is {np.sqrt(mean_squared_error(gt_scores,pred_scores))}\")\n",
        "\n",
        "                if mse_loss < best_loss:\n",
        "                    torch.save(self.state_dict(), \"model_1_\"  + str(epoch) + \".pth\")\n",
        "                    best_loss = mse_loss\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "        #self.bert_model = self.bert_model.roberta\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=0.001)\n",
        "\n",
        "        loss = nn.MSELoss()\n",
        "        train_dataloader = get_dataloaders_bert(self.train_file_path,self.model_type,'train',self.train_batch_size)\n",
        "\n",
        "        val_dataloader = get_dataloaders_bert(self.dev_file_path,self.model_type,\"val\",self.train_batch_size)\n",
        "        best_loss = sys.maxsize\n",
        "        best_accuracy = -sys.maxsize\n",
        "        steps = 0\n",
        "        pred_scores = []\n",
        "        gt_scores = []\n",
        "        print(f\"Pad token is {self.tokenizer.pad_token}\")\n",
        "        for epoch in range(self.epochs):\n",
        "            steps += 1\n",
        "            if epoch == 0:\n",
        "                scheduler = get_linear_schedule_with_warmup(optimizer,140,1400)\n",
        "            total_prev_loss = 0\n",
        "            for (batch_num, batch) in enumerate(train_dataloader):\n",
        "                # If gpu is available move to gpu.\n",
        "                if torch.cuda.is_available():\n",
        "                    input1 = batch[0].cuda()\n",
        "                    input2 = batch[1].cuda()\n",
        "                    locs = batch[2].cuda()\n",
        "                    gt = batch[3].cuda()\n",
        "                else:\n",
        "                    input1 = batch[0]\n",
        "                    input2 = batch[1]\n",
        "                    locs = batch[2]\n",
        "                    gt = batch[3]\n",
        "\n",
        "                loss_val = 0\n",
        "                self.bert_model.train()\n",
        "                self.attention.train()\n",
        "                self.linear_reg1.train()\n",
        "                self.prelu.train()\n",
        "                self.final_linear.train()\n",
        "\n",
        "                # Clear gradients\n",
        "                optimizer.zero_grad()\n",
        "                final_scores = self.forward((input1, input2,locs))\n",
        "                loss_val += loss(final_scores.squeeze(1), gt.float())\n",
        "                \n",
        "\n",
        "                # Compute gradients\n",
        "                loss_val.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
        "                total_prev_loss += loss_val.item()\n",
        "                print(\"Loss for batch\" + str(batch_num) + \": \" + str(loss_val.item()))\n",
        "                # Update weights according to the gradients computed.\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            # Don't compute gradients in validation step\n",
        "            with torch.no_grad():\n",
        "                # Ensure that dropout behavior is correct.\n",
        "                pred_scores = []\n",
        "                gt_scores = []\n",
        "                predictions = []\n",
        "                ground_truth = []\n",
        "                self.bert_model.eval()\n",
        "                self.attention.eval()\n",
        "                self.linear_reg1.eval()\n",
        "                self.final_linear.eval()\n",
        "                self.prelu.eval()\n",
        "                mse_loss = 0\n",
        "                for (val_batch_num, val_batch) in enumerate(val_dataloader):\n",
        "                    if torch.cuda.is_available():\n",
        "                        input1 = val_batch[0].cuda()\n",
        "                        input2 = val_batch[1].cuda()\n",
        "                        locs = val_batch[2].cuda()\n",
        "                        gt = val_batch[3].cuda()\n",
        "                    else:\n",
        "                        input1 = val_batch[0]\n",
        "                        input2 = val_batch[1]\n",
        "                        locs = val_batch[2]\n",
        "                        gt = val_batch[3]\n",
        "\n",
        "                    final_scores = self.forward((input1, input2, locs))\n",
        "                    pred_scores.extend(final_scores.cpu().detach().squeeze(1))\n",
        "                    gt_scores.extend(gt.cpu().detach())\n",
        "\n",
        "                    mse_loss += mean_squared_error(gt.cpu().detach(),final_scores.cpu().detach().squeeze(1))\n",
        "\n",
        "                    \n",
        "                print(f\"Validation Loss is {np.sqrt(mean_squared_error(gt_scores,pred_scores))}\")\n",
        "\n",
        "                if mse_loss < best_loss:\n",
        "                    torch.save(self.state_dict(), \"model_1_\"  + str(epoch) + \".pth\")\n",
        "                    best_loss = mse_loss\n",
        "      \n",
        "      \n",
        "    def predict(self, model_path=None):\n",
        "\n",
        "        '''\n",
        "        This function predicts the classes on a test set and outputs a csv file containing the id and predicted class\n",
        "        :param model_path: Path of the model to be loaded if not the current model is used.\n",
        "        :return:\n",
        "        '''\n",
        "\n",
        "        gts = []\n",
        "        preds = []\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "        if model_path:\n",
        "            self.load_state_dict(torch.load(model_path))\n",
        "        test_dataloader = get_sent_emb_dataloaders_bert(self.test_file_path, self.model_type,\"val\")\n",
        "        self.bert_model.eval()\n",
        "        self.linear_reg1.eval()\n",
        "        self.final_linear.eval()\n",
        "        self.prelu.eval()\n",
        "        self.attention.eval()\n",
        "        with torch.no_grad():\n",
        "            with open(\"task-1-output.csv\", \"w+\") as f:\n",
        "                f.writelines(\"id,pred\\n\")\n",
        "                for ind, batch in enumerate(test_dataloader):\n",
        "                    if torch.cuda.is_available():\n",
        "                        input1 = batch[0].cuda()\n",
        "                        input2 = batch[1].cuda()\n",
        "                        locs = batch[2].cuda()\n",
        "                        id = batch[4].cuda()\n",
        "                        gt = batch[3].cuda()\n",
        "                    else:\n",
        "                        input1 = batch[0]\n",
        "                        input2 = batch[1]\n",
        "                        locs = batch[2]\n",
        "                    final_scores_1 = self.forward((input1, input2, locs))\n",
        "                    preds.extend(final_scores_1.cpu().detach().squeeze(1))\n",
        "                    gts.extend(gt.cpu().detach())\n",
        "                    for cnt, pred in enumerate(final_scores_1):\n",
        "                        f.writelines(str(id[cnt].item()) + \",\" + str(pred.item()) + \"\\n\")\n",
        "                      \n",
        "                print(f\"Test score is {np.sqrt(mean_squared_error(gts,preds))}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BECChLfJgiGN",
        "colab_type": "code",
        "outputId": "ec608e9a-56fa-4a40-d5d4-9f1186ed0fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--batch_size\", action=\"store\", type=int, default=64, required=False)\n",
        "    parser.add_argument(\"--train_file_path\", type=str, default=\"../data/task-1/train.csv\", required=False)\n",
        "    parser.add_argument(\"--dev_file_path\", type=str, default=\"../data/task-1/dev.csv\", required=False)\n",
        "    parser.add_argument(\"--test_file_path\", type=str, default=\"../data/task-1/dev.csv\", required=False)\n",
        "    parser.add_argument(\"--lm_file_path\", type=str, default=\"../data/task-1/shortjokes2.csv\", required=False)\n",
        "    parser.add_argument(\"--lm_weights_file_path\", type=str, default=\"../models/lm_joke_bert.pth\", required=False)\n",
        "    parser.add_argument(\"--model_file_path\", type=str, default=\"../models/model_4.pth\", required=False)\n",
        "    parser.add_argument(\"--predict\", type=str, default='false', required=False)\n",
        "    parser.add_argument(\"--add_joke_train\", type=str, default='true', required=False)\n",
        "    parser.add_argument(\"--lm_pretrain\", type=str, default='false', required=False)\n",
        "    parser.add_argument(\"--word2vec\", type=str, default='false', required=False)\n",
        "    parser.add_argument(\"--joke_classification_path\", type=str, default='../data/task-1/joke_classification.csv',\n",
        "                        required=False)\n",
        "    parser.add_argument(\"--lr\", type=float, default=0.0001, required=False)\n",
        "    parser.add_argument(\"--train_scratch\", type=str, default='false', required=False)\n",
        "    parser.add_argument(\"--task\", type=int, default=1, required=False)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=5, required=False)\n",
        "    parser.add_argument(\"--model_type\", type=str', default='bert')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    obj = FunBERT(args.train_file_path, args.dev_file_path, args.test_file_path, args.lm_file_path, args.batch_size, 64,\n",
        "                args.lr, args.lm_weights_file_path, args.epochs, args.lm_pretrain,\n",
        "                args.model_file_path,args.model_type)\n",
        "\n",
        "    if args.lm_pretrain == 'true':\n",
        "        obj.pre_train_bert()\n",
        "\n",
        "    if args.predict == 'true':\n",
        "        obj.predict(args.model_file_path)\n",
        "    else:\n",
        "        # obj.train_joke_classification()\n",
        "        obj.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-e9204c753bce>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    parser.add_argument(\"--model_type\", type=str', default='bert')\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX7hDlOPuRtO",
        "colab_type": "text"
      },
      "source": [
        "To run the default Siamese architecture, use the cell below. Shortjokes2.csv is the file that contains the original haedlines for MLM training of Roberta. If you run the MLM, you need to uncomment lines corresponding to something like self.bert_model = self.bert_model.roberta to remove the language Modeling head."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFAFjmu9gwxb",
        "colab_type": "code",
        "outputId": "8b52b164-711c-4a86-e568-959e0e249d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "obj = FunBERT('train.csv', 'dev.csv', 'truth.csv', \n",
        "              'shortjokes2.csv', 64, 64,\n",
        "                2e-5, 'lm_joke_bert.pth', 5, None,'model_2.pth','roberta')\n",
        "#obj.bert_model = obj.bert_model.roberta\n",
        "#obj.bert_model.load_state_dict(torch.load('lm_joke_bert.pth'))\n",
        "obj.train()\n",
        "#obj.train_non_siamese()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "roberta\n",
            "['<s>', 'Ä France', 'Ä is', 'Ä Ã¢Ä¢', 'Äº', 'Ä hunting', 'Ä down', 'Ä its', 'Ä citizens', 'Ä who', 'Ä joined', 'Ä <', 'Ä Isis', 'Ä <', 'Ä Ã¢Ä¢', 'Ä»', 'Ä without', 'Ä trial', 'Ä in', 'Ä Iraq', '</s>']\n",
            "<pad>\n",
            "1\n",
            "[    0  1470    16    44   711  8217   159    63  2286    54  1770 28696\n",
            " 38931 28696    44    27   396  1500    11  3345     2     1     1     1\n",
            "     1     1     1     1     1     1     1     1     1     1     1     1\n",
            "     1     1     1     1     1     1     1     1     1     1     1     1\n",
            "     1     1]\n",
            "[11 13]\n",
            "roberta\n",
            "['<s>', 'Ä France', 'Ä is', 'Ä Ã¢Ä¢', 'Äº', 'Ä hunting', 'Ä down', 'Ä its', 'Ä citizens', 'Ä who', 'Ä joined', 'Ä ^', 'Ä twins', 'Ä ^', 'Ä Ã¢Ä¢', 'Ä»', 'Ä without', 'Ä trial', 'Ä in', 'Ä Iraq', '</s>']\n",
            "<pad>\n",
            "1\n",
            "[    0  1470    16    44   711  8217   159    63  2286    54  1770 37249\n",
            " 13137 37249    44    27   396  1500    11  3345     2     1     1     1\n",
            "     1     1     1     1     1     1     1     1     1     1     1     1\n",
            "     1     1     1     1     1     1     1     1     1     1     1     1\n",
            "     1     1]\n",
            "[11 13]\n",
            "roberta\n",
            "['<s>', 'Ä Thousands', 'Ä of', 'Ä gay', 'Ä and', 'Ä bisexual', 'Ä <', 'Ä men', 'Ä <', 'Ä convicted', 'Ä of', 'Ä long', '-', 'abol', 'ished', 'Ä sexual', 'Ä offences', 'Ä are', 'Ä post', 'hum', 'ously', 'Ä pard', 'oned', '</s>']\n",
            "<pad>\n",
            "1\n",
            "[    0 12233     9  5100     8 27100 28696   604 28696  3828     9   251\n",
            "    12 34342  6555  1363  9971    32   618 18257  9412 25427 11469     2\n",
            "     1     1     1     1     1     1     1     1     1     1     1     1\n",
            "     1     1     1     1     1     1     1     1     1     1     1     1\n",
            "     1     1]\n",
            "[6 8]\n",
            "roberta\n",
            "['<s>', 'Ä Thousands', 'Ä of', 'Ä gay', 'Ä and', 'Ä bisexual', 'Ä ^', 'Ä sw', 'ans', 'Ä ^', 'Ä convicted', 'Ä of', 'Ä long', '-', 'abol', 'ished', 'Ä sexual', 'Ä offences', 'Ä are', 'Ä post', 'hum', 'ously', 'Ä pard', 'oned', '</s>']\n",
            "<pad>\n",
            "1\n",
            "[    0 12233     9  5100     8 27100 37249  3514  1253 37249  3828     9\n",
            "   251    12 34342  6555  1363  9971    32   618 18257  9412 25427 11469\n",
            "     2     1     1     1     1     1     1     1     1     1     1     1\n",
            "     1     1     1     1     1     1     1     1     1     1     1     1\n",
            "     1     1]\n",
            "[6 9]\n",
            "Pad token is <pad>\n",
            "Loss for batch0: 2.0200366973876953\n",
            "Loss for batch1: 1.5371816158294678\n",
            "Loss for batch2: 1.4731941223144531\n",
            "Loss for batch3: 1.5695436000823975\n",
            "Loss for batch4: 1.7554638385772705\n",
            "Loss for batch5: 1.600090742111206\n",
            "Loss for batch6: 1.8082849979400635\n",
            "Loss for batch7: 1.7632508277893066\n",
            "Loss for batch8: 1.8121018409729004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-cfba4bba5377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#obj.bert_model = obj.bert_model.roberta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#obj.bert_model.load_state_dict(torch.load('lm_joke_bert.pth'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#obj.train_non_siamese()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-bad289bab349>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;31m# Compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0mtotal_prev_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPrSCU68ptl9",
        "colab_type": "code",
        "outputId": "222e7e9a-d5c4-4fe5-b07c-d7b3314faf1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr 29 02:24:35 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0    24W /  75W |   7587MiB /  7611MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-xb6Zu-pzjR",
        "colab_type": "code",
        "outputId": "1641837e-f9c8-4770-8e57-cf4ce18abd22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!ps -aux|grep python\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root          18  0.4  0.8 423952 116072 ?       Sl   02:11   0:03 /usr/bin/python2 /usr/local/bin/jupyter-notebook --ip=\"172.28.0.2\" --port=9000 --FileContentsManager.root_dir=\"/\" --MappingKernelManager.root_dir=\"/content\"\n",
            "root         123  7.9 39.6 29867260 5281776 ?    Ssl  02:12   0:56 /usr/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-a99e183f-09a7-478b-9768-161c3e3bbc42.json\n",
            "root         509  0.0  0.0  39192  6488 ?        S    02:24   0:00 /bin/bash -c ps -aux|grep python\n",
            "root         511  0.0  0.0  38568  5008 ?        S    02:24   0:00 grep python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd_UvOIZwa0Q",
        "colab_type": "text"
      },
      "source": [
        "If GPU is full and cannot free it despite clearing the class's object. Kill the process corresponding to the ipykernel process.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhfwH6zcvpHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 123"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzoPUBrOzaSD",
        "colab_type": "code",
        "outputId": "e99a5922-4d57-4531-e5c5-72fc799aeb27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "obj.predict('model_1_3.pth')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "roberta\n",
            "['<s>', 'Ä The', 'Ä Latest', 'Ä :', 'Ä Election', 'Ä tally', 'Ä shows', 'Ä <', 'Ä Austria', 'Ä <', 'Ä turning', 'Ä right', '</s>']\n",
            "<pad>\n",
            "1\n",
            "[    0    20  9385  4832  7713 11154   924 28696  9950 28696  3408   235\n",
            "     2     1     1     1     1     1     1     1     1     1     1     1\n",
            "     1     1     1     1     1     1     1     1     1     1     1     1\n",
            "     1     1     1     1     1     1     1     1     1     1     1     1\n",
            "     1     1]\n",
            "[7 9]\n",
            "roberta\n",
            "['<s>', 'Ä The', 'Ä Latest', 'Ä :', 'Ä Election', 'Ä tally', 'Ä shows', 'Ä ^', 'Ä Cars', 'Ä ^', 'Ä turning', 'Ä right', '</s>']\n",
            "<pad>\n",
            "1\n",
            "[    0    20  9385  4832  7713 11154   924 37249 17714 37249  3408   235\n",
            "     2     1     1     1     1     1     1     1     1     1     1     1\n",
            "     1     1     1     1     1     1     1     1     1     1     1     1\n",
            "     1     1     1     1     1     1     1     1     1     1     1     1\n",
            "     1     1]\n",
            "[7 9]\n",
            "Test score is 0.524755714997939\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}